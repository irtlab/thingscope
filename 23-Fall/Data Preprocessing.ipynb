{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16bd3722",
   "metadata": {},
   "source": [
    "# Public Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcfa6cb",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "- Convert all the flow txt files (output of Tranalyzer) into CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95155be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from 16-09-23_flows.txt written to 16-09-23.csv\n",
      "Data from 16-09-24_flows.txt written to 16-09-24.csv\n",
      "Data from 16-09-25_flows.txt written to 16-09-25.csv\n",
      "Data from 16-09-26_flows.txt written to 16-09-26.csv\n",
      "Data from 16-09-27_flows.txt written to 16-09-27.csv\n",
      "Data from 16-09-28_flows.txt written to 16-09-28.csv\n",
      "Data from 16-09-29_flows.txt written to 16-09-29.csv\n",
      "Data from 16-09-30_flows.txt written to 16-09-30.csv\n",
      "Data from 16-10-01_flows.txt written to 16-10-01.csv\n",
      "Data from 16-10-02_flows.txt written to 16-10-02.csv\n",
      "Data from 16-10-03_flows.txt written to 16-10-03.csv\n",
      "Data from 16-10-04_flows.txt written to 16-10-04.csv\n",
      "Data from 16-10-05_flows.txt written to 16-10-05.csv\n",
      "Data from 16-10-06_flows.txt written to 16-10-06.csv\n",
      "Data from 16-10-07_flows.txt written to 16-10-07.csv\n",
      "Data from 16-10-08_flows.txt written to 16-10-08.csv\n",
      "Data from 16-10-09_flows.txt written to 16-10-09.csv\n",
      "Data from 16-10-10_flows.txt written to 16-10-10.csv\n",
      "Data from 16-10-11_flows.txt written to 16-10-11.csv\n",
      "Data from 16-10-12_flows.txt written to 16-10-12.csv\n",
      "Data from 16-10-13_flows.txt written to 16-10-13.csv\n",
      "Data from 16-10-14_flows.txt written to 16-10-14.csv\n",
      "Data from 16-10-15_flows.txt written to 16-10-15.csv\n",
      "Data from 16-10-16_flows.txt written to 16-10-16.csv\n",
      "Data from 16-10-17_flows.txt written to 16-10-17.csv\n",
      "Data from 16-10-18_flows.txt written to 16-10-18.csv\n",
      "Data from 16-10-19_flows.txt written to 16-10-19.csv\n",
      "Data from 16-10-20_flows.txt written to 16-10-20.csv\n",
      "Data from 16-10-21_flows.txt written to 16-10-21.csv\n",
      "Data from 16-10-22_flows.txt written to 16-10-22.csv\n",
      "Data from 16-10-23_flows.txt written to 16-10-23.csv\n",
      "Data from 16-10-24_flows.txt written to 16-10-24.csv\n",
      "Data from 16-10-25_flows.txt written to 16-10-25.csv\n",
      "Data from 16-10-26_flows.txt written to 16-10-26.csv\n",
      "Data from 16-10-27_flows.txt written to 16-10-27.csv\n",
      "Data from 16-10-28_flows.txt written to 16-10-28.csv\n",
      "Data from 16-10-29_flows.txt written to 16-10-29.csv\n",
      "Data from 16-10-30_flows.txt written to 16-10-30.csv\n",
      "Data from 16-10-31_flows.txt written to 16-10-31.csv\n",
      "Data from 16-11-01_flows.txt written to 16-11-01.csv\n",
      "Data from 16-11-02_flows.txt written to 16-11-02.csv\n",
      "Data from 16-11-04_flows.txt written to 16-11-04.csv\n",
      "Data from 16-11-05_flows.txt written to 16-11-05.csv\n",
      "Data from 16-11-06_flows.txt written to 16-11-06.csv\n",
      "Data from 16-11-07_flows.txt written to 16-11-07.csv\n",
      "Data from 16-11-08_flows.txt written to 16-11-08.csv\n",
      "Data from 16-11-09_flows.txt written to 16-11-09.csv\n",
      "Data from 16-11-10_flows.txt written to 16-11-10.csv\n",
      "Data from 16-11-11_flows.txt written to 16-11-11.csv\n",
      "Data from 16-11-12_flows.txt written to 16-11-12.csv\n",
      "Data from 16-11-13_flows.txt written to 16-11-13.csv\n",
      "Data from 16-11-14_flows.txt written to 16-11-14.csv\n",
      "Data from 16-11-15_flows.txt written to 16-11-15.csv\n",
      "Data from 16-11-16_flows.txt written to 16-11-16.csv\n",
      "Data from 16-11-17_flows.txt written to 16-11-17.csv\n",
      "Data from 16-11-18_flows.txt written to 16-11-18.csv\n",
      "Data from 16-11-19_flows.txt written to 16-11-19.csv\n",
      "Data from 16-11-20_flows.txt written to 16-11-20.csv\n",
      "Data from 16-11-21_flows.txt written to 16-11-21.csv\n",
      "Data from 16-11-22_flows.txt written to 16-11-22.csv\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "directory = r'E:\\UNSW\\DNS_results'  # change the directory accordingly (output of Tranalyzer)\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('_flows.txt'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.read().splitlines()  # read the data from the text file\n",
    "\n",
    "        headers = lines[0].split(\"\\t\")  # headers\n",
    "        content = [line.split(\"\\t\") for line in lines[1:]]  # content\n",
    "\n",
    "        csv_filename = filename.replace('_flows.txt', '') + '.csv'  # corresponding csv file name\n",
    "        csv_file_path = os.path.join(directory, csv_filename)\n",
    "\n",
    "        # Convert the txt files to csv files\n",
    "        with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=',')\n",
    "            \n",
    "            # Write the data\n",
    "            writer.writerow(headers)\n",
    "            for row in content:\n",
    "                writer.writerow(row)\n",
    "\n",
    "        print(f\"Data from {filename} written to {csv_filename}\")\n",
    "\n",
    "print(\"All files processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbd2855",
   "metadata": {},
   "source": [
    "# Step 2: Data Cleaning - Only keep the following types of traffic\n",
    " - IoT devices communicate with endpoints outside the local network\n",
    " - DHCP and DNS when IoT devices communicate with the gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a33769eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data from 16-09-23.csv written to E:\\UNSW\\New_Filtered_data\\16-09-23.csv\n",
      "Filtered data from 16-09-24.csv written to E:\\UNSW\\New_Filtered_data\\16-09-24.csv\n",
      "Filtered data from 16-09-25.csv written to E:\\UNSW\\New_Filtered_data\\16-09-25.csv\n",
      "Filtered data from 16-09-26.csv written to E:\\UNSW\\New_Filtered_data\\16-09-26.csv\n",
      "Filtered data from 16-09-27.csv written to E:\\UNSW\\New_Filtered_data\\16-09-27.csv\n",
      "Filtered data from 16-09-28.csv written to E:\\UNSW\\New_Filtered_data\\16-09-28.csv\n",
      "Filtered data from 16-09-29.csv written to E:\\UNSW\\New_Filtered_data\\16-09-29.csv\n",
      "Filtered data from 16-09-30.csv written to E:\\UNSW\\New_Filtered_data\\16-09-30.csv\n",
      "Filtered data from 16-10-01.csv written to E:\\UNSW\\New_Filtered_data\\16-10-01.csv\n",
      "Filtered data from 16-10-02.csv written to E:\\UNSW\\New_Filtered_data\\16-10-02.csv\n",
      "Filtered data from 16-10-03.csv written to E:\\UNSW\\New_Filtered_data\\16-10-03.csv\n",
      "Filtered data from 16-10-04.csv written to E:\\UNSW\\New_Filtered_data\\16-10-04.csv\n",
      "Filtered data from 16-10-05.csv written to E:\\UNSW\\New_Filtered_data\\16-10-05.csv\n",
      "Filtered data from 16-10-06.csv written to E:\\UNSW\\New_Filtered_data\\16-10-06.csv\n",
      "Filtered data from 16-10-07.csv written to E:\\UNSW\\New_Filtered_data\\16-10-07.csv\n",
      "Filtered data from 16-10-08.csv written to E:\\UNSW\\New_Filtered_data\\16-10-08.csv\n",
      "Filtered data from 16-10-09.csv written to E:\\UNSW\\New_Filtered_data\\16-10-09.csv\n",
      "Filtered data from 16-10-10.csv written to E:\\UNSW\\New_Filtered_data\\16-10-10.csv\n",
      "Filtered data from 16-10-11.csv written to E:\\UNSW\\New_Filtered_data\\16-10-11.csv\n",
      "Filtered data from 16-10-12.csv written to E:\\UNSW\\New_Filtered_data\\16-10-12.csv\n",
      "Filtered data from 16-10-13.csv written to E:\\UNSW\\New_Filtered_data\\16-10-13.csv\n",
      "Filtered data from 16-10-14.csv written to E:\\UNSW\\New_Filtered_data\\16-10-14.csv\n",
      "Filtered data from 16-10-15.csv written to E:\\UNSW\\New_Filtered_data\\16-10-15.csv\n",
      "Filtered data from 16-10-16.csv written to E:\\UNSW\\New_Filtered_data\\16-10-16.csv\n",
      "Filtered data from 16-10-17.csv written to E:\\UNSW\\New_Filtered_data\\16-10-17.csv\n",
      "Filtered data from 16-10-18.csv written to E:\\UNSW\\New_Filtered_data\\16-10-18.csv\n",
      "Filtered data from 16-10-19.csv written to E:\\UNSW\\New_Filtered_data\\16-10-19.csv\n",
      "Filtered data from 16-10-20.csv written to E:\\UNSW\\New_Filtered_data\\16-10-20.csv\n",
      "Filtered data from 16-10-21.csv written to E:\\UNSW\\New_Filtered_data\\16-10-21.csv\n",
      "Filtered data from 16-10-22.csv written to E:\\UNSW\\New_Filtered_data\\16-10-22.csv\n",
      "Filtered data from 16-10-23.csv written to E:\\UNSW\\New_Filtered_data\\16-10-23.csv\n",
      "Filtered data from 16-10-24.csv written to E:\\UNSW\\New_Filtered_data\\16-10-24.csv\n",
      "Filtered data from 16-10-25.csv written to E:\\UNSW\\New_Filtered_data\\16-10-25.csv\n",
      "Filtered data from 16-10-26.csv written to E:\\UNSW\\New_Filtered_data\\16-10-26.csv\n",
      "Filtered data from 16-10-27.csv written to E:\\UNSW\\New_Filtered_data\\16-10-27.csv\n",
      "Filtered data from 16-10-28.csv written to E:\\UNSW\\New_Filtered_data\\16-10-28.csv\n",
      "Filtered data from 16-10-29.csv written to E:\\UNSW\\New_Filtered_data\\16-10-29.csv\n",
      "Filtered data from 16-10-30.csv written to E:\\UNSW\\New_Filtered_data\\16-10-30.csv\n",
      "Filtered data from 16-10-31.csv written to E:\\UNSW\\New_Filtered_data\\16-10-31.csv\n",
      "Filtered data from 16-11-01.csv written to E:\\UNSW\\New_Filtered_data\\16-11-01.csv\n",
      "Filtered data from 16-11-02.csv written to E:\\UNSW\\New_Filtered_data\\16-11-02.csv\n",
      "Filtered data from 16-11-04.csv written to E:\\UNSW\\New_Filtered_data\\16-11-04.csv\n",
      "Filtered data from 16-11-05.csv written to E:\\UNSW\\New_Filtered_data\\16-11-05.csv\n",
      "Filtered data from 16-11-06.csv written to E:\\UNSW\\New_Filtered_data\\16-11-06.csv\n",
      "Filtered data from 16-11-07.csv written to E:\\UNSW\\New_Filtered_data\\16-11-07.csv\n",
      "Filtered data from 16-11-08.csv written to E:\\UNSW\\New_Filtered_data\\16-11-08.csv\n",
      "Filtered data from 16-11-09.csv written to E:\\UNSW\\New_Filtered_data\\16-11-09.csv\n",
      "Filtered data from 16-11-10.csv written to E:\\UNSW\\New_Filtered_data\\16-11-10.csv\n",
      "Filtered data from 16-11-11.csv written to E:\\UNSW\\New_Filtered_data\\16-11-11.csv\n",
      "Filtered data from 16-11-12.csv written to E:\\UNSW\\New_Filtered_data\\16-11-12.csv\n",
      "Filtered data from 16-11-13.csv written to E:\\UNSW\\New_Filtered_data\\16-11-13.csv\n",
      "Filtered data from 16-11-14.csv written to E:\\UNSW\\New_Filtered_data\\16-11-14.csv\n",
      "Filtered data from 16-11-15.csv written to E:\\UNSW\\New_Filtered_data\\16-11-15.csv\n",
      "Filtered data from 16-11-16.csv written to E:\\UNSW\\New_Filtered_data\\16-11-16.csv\n",
      "Filtered data from 16-11-17.csv written to E:\\UNSW\\New_Filtered_data\\16-11-17.csv\n",
      "Filtered data from 16-11-18.csv written to E:\\UNSW\\New_Filtered_data\\16-11-18.csv\n",
      "Filtered data from 16-11-19.csv written to E:\\UNSW\\New_Filtered_data\\16-11-19.csv\n",
      "Filtered data from 16-11-20.csv written to E:\\UNSW\\New_Filtered_data\\16-11-20.csv\n",
      "Filtered data from 16-11-21.csv written to E:\\UNSW\\New_Filtered_data\\16-11-21.csv\n",
      "Filtered data from 16-11-22.csv written to E:\\UNSW\\New_Filtered_data\\16-11-22.csv\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Change the directory accordingly\n",
    "source_directory = r'E:\\UNSW\\DNS_results'\n",
    "destination_directory = r'E:\\UNSW\\New_Filtered_data'\n",
    "\n",
    "os.makedirs(destination_directory, exist_ok=True)\n",
    "\n",
    "# Non-IoT devices MAC addresses in UNSW\n",
    "Non_IoT_mac_addresses = {'40:f3:08:ff:1e:da', '74:2f:68:81:69:42', 'ac:bc:32:d4:6f:2f', \n",
    "                         'b4:ce:f6:a7:a3:c2', 'd0:a6:37:df:a1:e1', 'f4:5c:89:93:cc:85', \n",
    "                         '08:21:ef:3b:fc:e3', 'e8:ab:fa:19:de:4f', '30:8c:fb:b6:ea:45'}  # the last two are useless IoT devices in UNSW\n",
    "\n",
    "# IoT devices MAC addresses in UNSW\n",
    "IoT_mac_addresses = {'d0:52:a8:00:67:5e', '44:65:0d:56:cc:d3', '70:ee:50:18:34:43', 'f4:f2:6d:93:51:f1', \n",
    "                      '00:16:6c:ab:6b:88', '30:8c:fb:2f:e4:b2', '00:62:6e:51:27:2e', 'e8:ab:fa:19:de:4f', \n",
    "                      '00:24:e4:11:18:a8', 'ec:1a:59:79:f4:89', '50:c7:bf:00:56:39', '74:c6:3b:29:d7:1d', \n",
    "                      'ec:1a:59:83:28:11', '18:b4:30:25:be:e4', '70:ee:50:03:b8:ac', '00:24:e4:1b:6f:96', \n",
    "                      '74:6a:89:00:2e:25', '00:24:e4:20:28:c6', 'd0:73:d5:01:83:08', '18:b7:9e:02:20:44', \n",
    "                      'e0:76:d0:33:bb:85', '70:5a:0f:e4:9b:c0', '30:8c:fb:b6:ea:45'}\n",
    "\n",
    "# Data cleaning\n",
    "for filename in os.listdir(source_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(source_directory, filename)\n",
    "\n",
    "        df = pd.read_csv(file_path, low_memory=False)  # read the file into a dataframe\n",
    "\n",
    "        # Filter out Non-IoT device traffic\n",
    "        df_filtered = df[~df['srcMac'].isin(Non_IoT_mac_addresses) & ~df['dstMac'].isin(Non_IoT_mac_addresses)]\n",
    "\n",
    "        # Filter out traffic between IoT devices\n",
    "        df_filtered = df_filtered[~(df_filtered['srcMac'].isin(IoT_mac_addresses) & df_filtered['dstMac'].isin(IoT_mac_addresses))]\n",
    "\n",
    "        # Filter out flows with multiple MAC addresses\n",
    "        df_filtered = df_filtered[~df_filtered['srcMac'].str.contains(';') & ~df_filtered['dstMac'].str.contains(';')]\n",
    "        \n",
    "        # Only keep flows where either 'srcMac' or 'dstMac' is an IoT device\n",
    "        df_filtered = df_filtered[df_filtered['srcMac'].isin(IoT_mac_addresses) | df_filtered['dstMac'].isin(IoT_mac_addresses)]\n",
    "        \n",
    "        # ONLY KEEP TRAFFIC WITH OUTSIDE ENDPOINTS AND DHCP, DNS FLOWS\n",
    "        # Add a new column showing the endpoints' countries\n",
    "        df_filtered['IpCountry'] = df_filtered.apply(lambda row: row['dstIPCC'] if row['%dir'] == 'A' else row['srcIPCC'], axis=1)\n",
    "        \n",
    "        # Keep flows where IP country is not a number (meaning this flow is with the endpoint outside the local network) and DNS/DHCP flows\n",
    "        df_filtered = df_filtered[(df_filtered['IpCountry'].str.isnumeric() == False) & (df_filtered['IpCountry'] != \"-\") |\n",
    "                                  (df_filtered['dstPortClassN'] == 53) | (df_filtered['dstPortClassN'] == 67)]\n",
    "\n",
    "        filtered_csv_file_path = os.path.join(destination_directory, filename)\n",
    "        df_filtered.to_csv(filtered_csv_file_path, index=False)  # write the dataframe back to the CSV file\n",
    "\n",
    "        print(f\"Filtered data from {filename} written to {filtered_csv_file_path}\")\n",
    "\n",
    "print(\"All files processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6258ec",
   "metadata": {},
   "source": [
    "# Step 3: Add four new features for each IoT device\n",
    "- Number of visited endpoints\n",
    "- Number of visited NTP endpoints\n",
    "- Number of visited endpoints' countries\n",
    "- Number of visited NTP endpoints' countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2deae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# Calculate the number of visited (NTP) endpoints - by DOMAIN NAMES rather than IP addresses\n",
    "def calculate_num_of_endpoints(df, IoT_mac_addresses):\n",
    "    unique_info_per_srcMac = {}  # store the new features for each unqiue IoT device (per srcMac)\n",
    "\n",
    "    # Iterate over each unique srcMac (each IoT device)\n",
    "    for srcMac in df['srcMac'].unique():\n",
    "        if srcMac not in IoT_mac_addresses:\n",
    "            continue\n",
    "        \n",
    "        # Initialize sets to store features for this IoT device\n",
    "        unique_tuples_set = set()\n",
    "        unique_dnsQnames = set()\n",
    "        unique_dnsQnames_with_ntp = set()\n",
    "\n",
    "        # Find unique visited dstIPs for each IoT device\n",
    "        unique_dstIPs = df[df['srcMac'] == srcMac]['dstIP'].unique()\n",
    "\n",
    "        # Iterate over each unique dstIP\n",
    "        for dstIP in unique_dstIPs:\n",
    "            # Exclude dstIPs starting with 192.168.1 and 0.0.0.0\n",
    "            if dstIP.startswith(\"192.168.1\") or dstIP == \"0.0.0.0\":\n",
    "                continue\n",
    "\n",
    "            # Extract the corresponding dstPortClass for this dstIP\n",
    "            dstPortClass_rows = df[df['dstIP'] == dstIP]['dstPortClass']\n",
    "            dstPortClass = dstPortClass_rows.iloc[0] if not dstPortClass_rows.empty else None\n",
    "\n",
    "            # Check if this dstIP is in any dns4Aaddress or dns6Aaddress from DNS flows\n",
    "            matched_dns4 = df[df['dns4Aaddress'].str.contains(dstIP, na=False)]\n",
    "            matched_dns6 = df[df['dns6Aaddress'].str.contains(dstIP, na=False)]\n",
    "            matched_dns = pd.concat([matched_dns4, matched_dns6]).drop_duplicates()\n",
    "\n",
    "            if not matched_dns.empty:\n",
    "                # If found, process each match\n",
    "                for _, row in matched_dns.iterrows():\n",
    "                    dnsQname = row['dnsQname']\n",
    "                    dstMac = row['dstMac']\n",
    "                    \n",
    "                    # Process tuples only when this DNS flow belongs to the corresponding IoT device\n",
    "                    if srcMac == dstMac:\n",
    "                        tuple_to_print = (srcMac, dstIP, dnsQname, dstMac, dstPortClass)\n",
    "                        if tuple_to_print not in unique_tuples_set:\n",
    "                            unique_tuples_set.add(tuple_to_print)\n",
    "                            unique_dnsQnames.add(dnsQname)\n",
    "                            \n",
    "                            # Count NTP endpoints\n",
    "                            if dstPortClass == \"ntp\":\n",
    "                                unique_dnsQnames_with_ntp.add(dnsQname)\n",
    "\n",
    "        # Store the number of visited domain names and the number of visited NTP domain names for each IoT devcie\n",
    "        unique_info_per_srcMac[srcMac] = {\n",
    "            'total_unique_dnsQnames': len(unique_dnsQnames),\n",
    "            'unique_dnsQnames_with_ntp': len(unique_dnsQnames_with_ntp)\n",
    "        }\n",
    "    \n",
    "    return unique_info_per_srcMac\n",
    "\n",
    "\n",
    "# Calculate the number of visited endpoints' countries\n",
    "def calculate_num_of_endpoints_countries(df, IoT_mac_addresses):\n",
    "    # Create a new column for grouping, using dstMac if srcMac is not the IoT device\n",
    "    df['group_mac'] = df.apply(lambda row: row['dstMac'] if row['srcMac'] not in IoT_mac_addresses else row['srcMac'], axis=1)\n",
    "\n",
    "    filtered_df = df[(df['dstIPCC'] != '-') & (~df['dstIPCC'].str.isnumeric())]\n",
    "    num_endpoints_countries = filtered_df.groupby('group_mac')['dstIPCC'].nunique().reset_index(name='Num_of_endpoints_countries')\n",
    "\n",
    "    result = df.merge(num_endpoints_countries, on='group_mac', how='left')\n",
    "    result['Num_of_endpoints_countries'] = result['Num_of_endpoints_countries'].fillna(0).astype(int)\n",
    "    \n",
    "    return result.drop(columns=['group_mac'])\n",
    "\n",
    "\n",
    "# Calculate the number of visited NTP endpoints' countries\n",
    "def calculate_num_of_ntp_endpoints_countries(df, IoT_mac_addresses):\n",
    "    # Create a new column for grouping, using dstMac if srcMac is not the IoT device\n",
    "    df['group_mac'] = df.apply(lambda row: row['dstMac'] if row['srcMac'] not in IoT_mac_addresses else row['srcMac'], axis=1)\n",
    "\n",
    "    filtered_df = df[(df['dstPortClass'] == 'ntp') & (df['dstIPCC'] != '-') & (~df['dstIPCC'].str.isnumeric())]  # check for NTP endpoints\n",
    "    num_ntp_endpoints_countries = filtered_df.groupby('group_mac')['dstIPCC'].nunique().reset_index(name='Num_of_ntp_endpoints_countries')\n",
    "\n",
    "    result = df.merge(num_ntp_endpoints_countries, on='group_mac', how='left')\n",
    "    result['Num_of_ntp_endpoints_countries'] = result['Num_of_ntp_endpoints_countries'].fillna(0).astype(int)\n",
    "    \n",
    "    return result.drop(columns=['group_mac'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225a3d20",
   "metadata": {},
   "source": [
    "# Step 4: Data Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c55e8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label each flow, mapping each one to an IoT device\n",
    "def label_data(df, mapping, IoT_mac_addresses):\n",
    "    \n",
    "    def get_label(mac):\n",
    "        return mapping.get(mac, None)\n",
    "    \n",
    "    # Label the flow either by srcMac or dstMac\n",
    "    df['label'] = df.apply(lambda row: get_label(row['srcMac']) if row['srcMac'] in IoT_mac_addresses else get_label(row['dstMac']), axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4061eacf",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f96b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Change the directories accordingly\n",
    "    source_directory = r'E:\\UNSW\\New_Filtered_data'\n",
    "    destination_directory = r'E:\\UNSW\\Processed_data'\n",
    "    \n",
    "    os.makedirs(destination_directory, exist_ok=True)\n",
    "    \n",
    "    # IoT MAC addresses\n",
    "    IoT_mac_addresses = {'d0:52:a8:00:67:5e', '44:65:0d:56:cc:d3', '70:ee:50:18:34:43', 'f4:f2:6d:93:51:f1', \n",
    "                         '00:16:6c:ab:6b:88', '30:8c:fb:2f:e4:b2', '00:62:6e:51:27:2e', '00:24:e4:11:18:a8', \n",
    "                         'ec:1a:59:79:f4:89', '50:c7:bf:00:56:39', '74:c6:3b:29:d7:1d', 'ec:1a:59:83:28:11', \n",
    "                         '18:b4:30:25:be:e4', '70:ee:50:03:b8:ac', '00:24:e4:1b:6f:96', '74:6a:89:00:2e:25', \n",
    "                         '00:24:e4:20:28:c6', 'd0:73:d5:01:83:08', '18:b7:9e:02:20:44', 'e0:76:d0:33:bb:85', \n",
    "                         '70:5a:0f:e4:9b:c0'}\n",
    "    \n",
    "    # Map each IoT device to a label\n",
    "    label_mapping = {\n",
    "        'd0:52:a8:00:67:5e': 0,\n",
    "        '44:65:0d:56:cc:d3': 1,\n",
    "        '70:ee:50:18:34:43': 2,\n",
    "        'f4:f2:6d:93:51:f1': 3,\n",
    "        '00:16:6c:ab:6b:88': 4,\n",
    "        '30:8c:fb:2f:e4:b2': 5,\n",
    "        '00:62:6e:51:27:2e': 6,\n",
    "        '00:24:e4:11:18:a8': 7,\n",
    "        'ec:1a:59:79:f4:89': 8,\n",
    "        '50:c7:bf:00:56:39': 9,\n",
    "        '74:c6:3b:29:d7:1d': 10,\n",
    "        'ec:1a:59:83:28:11': 11,\n",
    "        '18:b4:30:25:be:e4': 12,\n",
    "        '70:ee:50:03:b8:ac': 13,\n",
    "        '00:24:e4:1b:6f:96': 14,\n",
    "        '74:6a:89:00:2e:25': 15,\n",
    "        '00:24:e4:20:28:c6': 16,\n",
    "        'd0:73:d5:01:83:08': 17,\n",
    "        '18:b7:9e:02:20:44': 18,\n",
    "        'e0:76:d0:33:bb:85': 19,\n",
    "        '70:5a:0f:e4:9b:c0': 20\n",
    "    }\n",
    "    \n",
    "    for filename in os.listdir(source_directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(source_directory, filename)\n",
    "    \n",
    "            df = pd.read_csv(file_path, low_memory=False)  # read the csv file as a dataframe\n",
    "            \n",
    "            df = label_data(df, label_mapping, IoT_mac_addresses)  # label the data\n",
    "            \n",
    "            unique_info = calculate_num_of_endpoints(df, IoT_mac_addresses) # calculate the number of visited (NTP) endpoints\n",
    "\n",
    "            # Add new columns to the original dataframe, based on the srcMac or dstMac\n",
    "            df['Number_of_endpoints'] = df.apply(lambda row: unique_info.get(row['srcMac'], {'total_unique_dstIPs': 0})['total_unique_dstIPs'] if row['srcMac'] in IoT_mac_addresses else unique_info.get(row['dstMac'], {'total_unique_dstIPs': 0})['total_unique_dstIPs'], axis=1)\n",
    "            df['Number_of_ntp_endpoints'] = df.apply(lambda row: unique_info.get(row['srcMac'], {'unique_ntp_dstIPs': 0})['unique_ntp_dstIPs'] if row['srcMac'] in IoT_mac_addresses else unique_info.get(row['dstMac'], {'unique_ntp_dstIPs': 0})['unique_ntp_dstIPs'], axis=1)\n",
    "\n",
    "            df = calculate_num_of_endpoints_countries(df, IoT_mac_addresses)  # calculate the number of visited endpoints' countries\n",
    "            df = calculate_num_of_ntp_endpoints_countries(df, IoT_mac_addresses)  # calculate the number of visited NTP endpoints' countries\n",
    "            \n",
    "            # Delete useless columns\n",
    "            columns_to_delete = ['dnsStat', 'dnsHdrOPField', 'dnsHFlg_OpC_RetC', 'dnsCntQu_Asw_Aux_Add', 'dnsAAAqF', 'dnsQname', 'dnsAname', 'dnsAPname', 'dns4Aaddress', 'dns6Aaddress', 'dnsQType', 'dnsQClass', 'dnsAType', 'dnsAClass', 'dnsATTL', 'dnsMXpref', 'dnsSRVprio', 'dnsSRVwgt', 'dnsSRVprt', 'dnsOptStat', 'IpCountry']\n",
    "            df.drop(columns=columns_to_delete, inplace=True)\n",
    "\n",
    "            new_csv_path = os.path.join(destination_directory, filename)\n",
    "            df.to_csv(new_csv_path, index=False)  # updated csv files\n",
    "        \n",
    "            print(f\"Processed {filename} and saved to {new_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6830d120",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 16-09-23.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-09-23.csv\n",
      "Processed 16-09-24.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-09-24.csv\n",
      "Processed 16-09-25.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-09-25.csv\n",
      "Processed 16-09-26.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-09-26.csv\n",
      "Processed 16-09-27.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-09-27.csv\n",
      "Processed 16-09-28.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-09-28.csv\n",
      "Processed 16-09-29.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-09-29.csv\n",
      "Processed 16-09-30.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-09-30.csv\n",
      "Processed 16-10-01.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-01.csv\n",
      "Processed 16-10-02.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-02.csv\n",
      "Processed 16-10-03.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-03.csv\n",
      "Processed 16-10-04.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-04.csv\n",
      "Processed 16-10-05.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-05.csv\n",
      "Processed 16-10-06.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-06.csv\n",
      "Processed 16-10-07.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-07.csv\n",
      "Processed 16-10-08.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-08.csv\n",
      "Processed 16-10-09.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-09.csv\n",
      "Processed 16-10-10.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-10.csv\n",
      "Processed 16-10-11.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-11.csv\n",
      "Processed 16-10-12.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-12.csv\n",
      "Processed 16-10-13.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-13.csv\n",
      "Processed 16-10-14.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-14.csv\n",
      "Processed 16-10-15.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-15.csv\n",
      "Processed 16-10-16.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-16.csv\n",
      "Processed 16-10-17.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-17.csv\n",
      "Processed 16-10-18.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-18.csv\n",
      "Processed 16-10-19.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-19.csv\n",
      "Processed 16-10-20.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-20.csv\n",
      "Processed 16-10-21.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-21.csv\n",
      "Processed 16-10-22.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-22.csv\n",
      "Processed 16-10-23.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-23.csv\n",
      "Processed 16-10-24.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-24.csv\n",
      "Processed 16-10-25.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-25.csv\n",
      "Processed 16-10-26.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-26.csv\n",
      "Processed 16-10-27.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-27.csv\n",
      "Processed 16-10-28.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-28.csv\n",
      "Processed 16-10-29.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-29.csv\n",
      "Processed 16-10-30.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-30.csv\n",
      "Processed 16-10-31.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-10-31.csv\n",
      "Processed 16-11-01.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-01.csv\n",
      "Processed 16-11-02.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-02.csv\n",
      "Processed 16-11-04.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-04.csv\n",
      "Processed 16-11-05.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-05.csv\n",
      "Processed 16-11-06.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-06.csv\n",
      "Processed 16-11-07.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-07.csv\n",
      "Processed 16-11-08.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-08.csv\n",
      "Processed 16-11-09.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-09.csv\n",
      "Processed 16-11-10.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-10.csv\n",
      "Processed 16-11-11.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-11.csv\n",
      "Processed 16-11-12.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-12.csv\n",
      "Processed 16-11-13.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-13.csv\n",
      "Processed 16-11-14.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-14.csv\n",
      "Processed 16-11-15.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-15.csv\n",
      "Processed 16-11-16.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-16.csv\n",
      "Processed 16-11-17.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-17.csv\n",
      "Processed 16-11-18.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-18.csv\n",
      "Processed 16-11-19.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-19.csv\n",
      "Processed 16-11-20.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-20.csv\n",
      "Processed 16-11-21.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-21.csv\n",
      "Processed 16-11-22.csv and saved to E:\\UNSW\\Processed_data_endpoints_IP\\16-11-22.csv\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
